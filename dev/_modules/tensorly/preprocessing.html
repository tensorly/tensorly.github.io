<!DOCTYPE html>

<html lang="en" data-content_root="../../">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>tensorly.preprocessing &#8212; TensorLy: Tensor Learning in Python</title> 
<link rel="stylesheet" href="../../_static/tensorly_style.css">
<link rel="apple-touch-icon" sizes="180x180" href="../../_static/favicon/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="../../_static/favicon/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="../../_static/favicon/favicon-16x16.png">
<link rel="manifest" href="../../_static/favicon/site.webmanifest">
<link rel="mask-icon" href="../../_static/favicon/safari-pinned-tab.svg" color="#5bbad5">
<link rel="shortcut icon" href="../../_static/favicon/favicon.ico">
<meta name="theme-color" content="#ffffff">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="../../_static/tensorly_style.css?v=a02e9698" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery.css?v=61a4c737" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../../_static/sg_gallery-rendered-html.css?v=1277b6f3" />

  
    <script src="../../_static/documentation_options.js?v=ec16d22d"></script>
    <script src="../../_static/doctools.js?v=888ff710"></script>
    <script src="../../_static/sphinx_highlight.js?v=dc90522c"></script>
 <script src="../../_static/navbar_burger.js"></script>
 <script defer src="https://use.fontawesome.com/releases/v5.14.0/js/all.js"></script>
 
<script async src="https://www.googletagmanager.com/gtag/js?id=G-3V91QCZR03"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-3V91QCZR03');
</script>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">

  </head>
<body  class="has-navbar-fixed-top">

  <header>
    <navbar>
      <nav class="navbar top-navbar is-fixed-top has-shadow is-flex-wrap-wrap" role="navigation" aria-label="main top navigation">
        <div class="navbar-brand">
        

          <a class="navbar-item" href="../../index.html">
            <img src="../../_static/logo_tensorly.png" height="28">
          </a>
          <a class="navbar-item is-hidden-desktop" href="https://github.com/tensorly/tensorly" target="_blank">
              <span class="icon"><i class="fab fa-github"></i></span>
          </a>

          <a role="button" class="navbar-burger" data-target="top-nav-menu" aria-label="menu" aria-expanded="false">
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
            <span aria-hidden="true"></span>
          </a>

        </div>
        
        <div class="navbar-menu" id="top-nav-menu">
        

          <div class="navbar-start">
            
              <a class="navbar-item" href="../../installation.html">
              Install
            </a>
              <a class="navbar-item" href="../../user_guide/index.html">
              User Guide
            </a>
              <a class="navbar-item" href="../../modules/api.html">
              API
            </a>
              <a class="navbar-item" href="../../auto_examples/index.html">
              Examples
            </a>
              <a class="navbar-item" href="../../about.html">
              About Us
            </a>
            <div class="navbar-item has-dropdown is-hoverable is-boxed">
              <a class="navbar-link">
                Ecosystem
              </a>
              <div class="navbar-dropdown top-navbar">
                <a class="navbar-item" href="http://tensorly.org/torch" target="_blank">
                  TensorLy-Torch
                </a>
                <a class="navbar-item" href="http://tensorly.org/quantum" target="_blank">
                  TensorLy-Quantum
                </a>
                <a class="navbar-item" href="http://tensorly.org/viz" target="_blank">
                  TensorLy-Viz
                </a>
                <a class="navbar-item" href="https://github.com/JeanKossaifi/tensorly-notebooks" target="_blank">
                  Notebooks
                </a>
              </div>
            </div>
          </div>
        
          <div class="navbar-end">
            <div class="navbar-item">
            
            <a class="button is-hidden-touch is-dark" href="https://github.com/tensorly/tensorly" target="_blank">
              <span class="icon-text">
                <span class="icon is-large">
                  <i class="fab fa-github"></i>
                </span>
                <span>Github</span>
              </span>
            </a>

            </div> 
          </div> 
        </div> 

      </nav>
      
    </navbar>
  </header>


  <div id="column-container">
  <div class="columns is-mobile is-centered">
	
  

  <div class="column main-column">

    
    <div class="main-section">

      
      

      <div class="container content main-content">
        
  <h1>Source code for tensorly.preprocessing</h1><div class="highlight"><pre>
<span></span><span class="kn">from</span> <span class="nn">tensorly</span> <span class="kn">import</span> <span class="n">backend</span> <span class="k">as</span> <span class="n">T</span>

<span class="kn">from</span> <span class="nn">.parafac2_tensor</span> <span class="kn">import</span> <span class="n">Parafac2Tensor</span>
<span class="kn">from</span> <span class="nn">.tenalg.svd</span> <span class="kn">import</span> <span class="n">svd_interface</span>


<div class="viewcode-block" id="svd_compress_tensor_slices">
<a class="viewcode-back" href="../../modules/generated/tensorly.preprocessing.svd_compress_tensor_slices.html#tensorly.preprocessing.svd_compress_tensor_slices">[docs]</a>
<span class="k">def</span> <span class="nf">svd_compress_tensor_slices</span><span class="p">(</span>
    <span class="n">tensor_slices</span><span class="p">,</span> <span class="n">compression_threshold</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_rank</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">svd</span><span class="o">=</span><span class="s2">&quot;truncated_svd&quot;</span>
<span class="p">):</span>
<span class="w">    </span><span class="sa">r</span><span class="sd">&quot;&quot;&quot;Compress data with the SVD for running PARAFAC2.</span>

<span class="sd">    PARAFAC2 can be sped up massively for data where the number of rows in the tensor slices</span>
<span class="sd">    is much greater than their rank. In that case, we can compress the data by computing the</span>
<span class="sd">    SVD and fitting the PARAFAC2 model to the right singular vectors multiplied by the singular</span>
<span class="sd">    values. Then, we can &quot;decompress&quot; the decomposition by left-multiplying the :math:`B_i`-matrices</span>
<span class="sd">    by the left singular values to get a decomposition as if it was fitted to the uncompressed</span>
<span class="sd">    data. We can essentially think of this as running a PCA without centering the data for each</span>
<span class="sd">    tensor slice and fitting the PARAFAC2 model to the scores. Then, to get back the components,</span>
<span class="sd">    we left-multiply the :math:`B_i`-matrices with the loading matrices.</span>

<span class="sd">    [1]_ states that we can constrain our :math:`B_i`-matrices to lie in a given vector space,</span>
<span class="sd">    :math:`\mathscr{V}_i` by multiplying the data matrices with an orthogonal basis matrix that</span>
<span class="sd">    spans :math:`\mathscr{V}_i`. However, since we know that :math:`B_i` lie in the column space</span>
<span class="sd">    of :math:`X_i`, we can multiply the :math:`X_i`-matrices by an orthogonal matrix that spans</span>
<span class="sd">    :math:`\text{col}(X_i)` without affecting the fit of the model. Thus we can compress our data</span>
<span class="sd">    prior to fitting the PARAFAC2 model whenever the number of rows in our data matrices exceeds</span>
<span class="sd">    the number of columns (as the rank of :math:`\text{col}(X_i)` cannot exceed the number of rows).</span>

<span class="sd">    To implement this, we use the SVD to get an orthogonal basis for the column space of :math:`X_i`.</span>
<span class="sd">    Moreover, since :math:`S_i V_i^T = U_i^T X_i`, we can skip an additional matrix multiplication</span>
<span class="sd">    by fitting the model to :math:`S_i V_i^T`.</span>

<span class="sd">    Finally, we note that this approach can also be implemented by truncating the SVD. If an appropriate</span>
<span class="sd">    threshold is set, this will not affect the fitted model in any major form.</span>

<span class="sd">    .. note::</span>
<span class="sd">        This can be thought of as a simplified version of the DPAR approach for compressing PARAFAC2 models [2]_,</span>
<span class="sd">        which compresses all modes of :math:`\mathcal{X}` to fit an approximate PARAFAC2 model.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    tensor_slices : list of matrices</span>
<span class="sd">        The data matrices to compress.</span>
<span class="sd">    compression_threshold : float (0 &lt;= compression_threshold &lt;= 1)</span>
<span class="sd">        Threshold at which the singular values should be truncated. Any singular value less than</span>
<span class="sd">        compression_threshold * s[0] is set to zero. Note that if this is nonzero, then the found</span>
<span class="sd">        components will likely be affected.</span>
<span class="sd">    max_rank : int</span>
<span class="sd">        The maximum rank to allow in the datasets after compression. This also serves to speed up</span>
<span class="sd">        the SVD calculation with matrices containing many rows and columns when paired with randomized</span>
<span class="sd">        SVD solving.</span>
<span class="sd">    svd : str, default is &#39;truncated_svd&#39;</span>
<span class="sd">        Function to use to compute the SVD, acceptable values in tensorly.SVD_FUNS</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    list of matrices</span>
<span class="sd">        The score matrices, used to fit the PARAFAC2 model to.</span>
<span class="sd">    list of matrices</span>
<span class="sd">        The loading matrices, used to decompress the PARAFAC2 components after fitting</span>
<span class="sd">        to the scores.</span>

<span class="sd">    References</span>
<span class="sd">    ----------</span>
<span class="sd">    .. [1] Helwig, N. E. (2017). Estimating latent trends in multivariate longitudinal</span>
<span class="sd">           data via Parafac2 with functional and structural constraints. Biometrical</span>
<span class="sd">           Journal, 59(4), 783-803. doi: 10.1002/bimj.201600045</span>

<span class="sd">    .. [2] Jang JG, Kang U. Dpar2: Fast and scalable parafac2 decomposition for</span>
<span class="sd">           irregular dense tensors. 38th International Conference on Data Engineering</span>
<span class="sd">           (ICDE) 2022 May 9 (pp. 2454-2467). IEEE.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">loading_matrices</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tensor_slices</span><span class="p">]</span>
    <span class="n">score_matrices</span> <span class="o">=</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="n">tensor_slices</span><span class="p">]</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">n_cols</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor_slices</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>

    <span class="k">if</span> <span class="n">max_rank</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">rank_limit</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">n_cols</span><span class="p">,</span> <span class="n">max_rank</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">rank_limit</span> <span class="o">=</span> <span class="n">n_cols</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor_slice</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">tensor_slices</span><span class="p">):</span>
        <span class="n">n_rows</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">tensor_slice</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">n_rows</span> <span class="o">&lt;=</span> <span class="n">rank_limit</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">compression_threshold</span><span class="p">:</span>
            <span class="n">score_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor_slice</span>
            <span class="k">continue</span>

        <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">svd_interface</span><span class="p">(</span><span class="n">tensor_slice</span><span class="p">,</span> <span class="n">n_eigenvecs</span><span class="o">=</span><span class="n">rank_limit</span><span class="p">,</span> <span class="n">method</span><span class="o">=</span><span class="n">svd</span><span class="p">)</span>

        <span class="c1"># Threshold SVD, keeping only singular values that satisfy s_i &gt;= s_0 * epsilon</span>
        <span class="c1"># where epsilon is the compression threshold</span>
        <span class="n">num_svds</span> <span class="o">=</span> <span class="nb">len</span><span class="p">([</span><span class="n">s_i</span> <span class="k">for</span> <span class="n">s_i</span> <span class="ow">in</span> <span class="n">s</span> <span class="k">if</span> <span class="n">s_i</span> <span class="o">&gt;=</span> <span class="p">(</span><span class="n">s</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="n">compression_threshold</span><span class="p">)])</span>
        <span class="n">U</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">Vh</span> <span class="o">=</span> <span class="n">U</span><span class="p">[:,</span> <span class="p">:</span><span class="n">num_svds</span><span class="p">],</span> <span class="n">s</span><span class="p">[:</span><span class="n">num_svds</span><span class="p">],</span> <span class="n">Vh</span><span class="p">[:</span><span class="n">num_svds</span><span class="p">,</span> <span class="p">:]</span>

        <span class="c1"># Array broadcasting happens at the last dimension, since Vh is num_svds x n_cols</span>
        <span class="c1"># we need to transpose it, multiply in the singular values and then transpose</span>
        <span class="c1"># it again. This is equivalent to writing diag(s) @ Vh. If we skip the</span>
        <span class="c1"># transposes, we would get Vh @ diag(s), which is wrong.</span>
        <span class="n">score_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">s</span> <span class="o">*</span> <span class="n">T</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">Vh</span><span class="p">))</span>
        <span class="n">loading_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">U</span>

    <span class="k">return</span> <span class="n">score_matrices</span><span class="p">,</span> <span class="n">loading_matrices</span></div>



<div class="viewcode-block" id="svd_decompress_parafac2_tensor">
<a class="viewcode-back" href="../../modules/generated/tensorly.preprocessing.svd_decompress_parafac2_tensor.html#tensorly.preprocessing.svd_decompress_parafac2_tensor">[docs]</a>
<span class="k">def</span> <span class="nf">svd_decompress_parafac2_tensor</span><span class="p">(</span><span class="n">parafac2_tensor</span><span class="p">,</span> <span class="n">loading_matrices</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Decompress the factors obtained by fitting PARAFAC2 on SVD-compressed data</span>

<span class="sd">    Decompress a PARAFAC2 decomposition that describes the compressed data so that it</span>
<span class="sd">    models the original uncompressed data. Fitting to compressed data, and then</span>
<span class="sd">    decompressing is mathematically equivalent to fitting to the uncompressed data.</span>

<span class="sd">    See :py:meth:`svd_compress_tensor_slices` for information about SVD-compression and</span>
<span class="sd">    decompression.</span>

<span class="sd">    .. note::</span>
<span class="sd">        To decompress the data, we left-multiply the loading-matrices into the</span>
<span class="sd">        :math:`B_i`-matrices. However, :math:`B_i = P_i B`, so the decompression is</span>
<span class="sd">        implemented by left-multiplying the loading matrices by the :math:`P_i`-matrices.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    parafac2_tensor: tl.Parafac2Tensor</span>
<span class="sd">        A decomposition obtained from fitting a PARAFAC2 model to compressed data</span>
<span class="sd">    loading_matrices: list of matrices</span>
<span class="sd">        Loading matrices obtained when compressing the data. See</span>
<span class="sd">        :py:meth:`svd_compress_tensor_slices` for more information.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    tl.Parafac2Tensor:</span>
<span class="sd">        Decompressed PARAFAC2 decomposition - equivalent to the decomposition we would</span>
<span class="sd">        get from fitting parafac2 to uncompressed data.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weights</span><span class="p">,</span> <span class="n">factors</span><span class="p">,</span> <span class="n">projections</span> <span class="o">=</span> <span class="n">parafac2_tensor</span>
    <span class="n">projections</span> <span class="o">=</span> <span class="n">projections</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">projection</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">projections</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">loading_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">projections</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">T</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">loading_matrices</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">projection</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">Parafac2Tensor</span><span class="p">((</span><span class="n">weights</span><span class="p">,</span> <span class="n">factors</span><span class="p">,</span> <span class="n">projections</span><span class="p">))</span></div>

</pre></div>

      </div>

      

        <footer class="footer">
    <div class="content has-text-centered">
        <div class="block">
          &copy; Copyright 2016 - 2023, TensorLy Developers.<br/>
        </div>
    </div>
  </footer>

    </div>

  </div>  

	

  </div>  
  </div> 

  
  <script>
    function toggle_sidebar() {
        var element = document.getElementById("sidebar");
        var container = document.getElementById("column-container");
        var localtoccolumn = document.getElementById("localtoc-column");
        element.classList.toggle("hide-tablet");
        element.classList.toggle("is-hidden-mobile");
        container.classList.toggle("sidemenu-hidden");
        localtoccolumn.classList.toggle("is-one-fifth-widescreen");
        localtoccolumn.classList.toggle("is-2-desktop");
        localtoccolumn.classList.toggle("is-3-desktop");
    }
  </script> 



  </body>
</html>
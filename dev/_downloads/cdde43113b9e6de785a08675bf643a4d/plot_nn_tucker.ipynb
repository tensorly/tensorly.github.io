{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Non-negative Tucker decomposition in Tensorly >=0.6\nExample and comparison of Non-negative Tucker decompositions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\nSince version 0.6 in Tensorly, two algorithms are available to compute non-negative\nTucker decomposition:\n\n1. Multiplicative updates (MU) (already in Tensorly < 0.6)\n2. Non-negative Alternating Least Squares (ALS) using Hierarchical ALS (HALS)\n\nNon-negativity is an important constraint to handle for tensor decompositions.\nOne could expect that core and factors must have only non-negative values after\nit is obtained from a non-negative tensor. Tucker decomposition includes core\n($G$) and factors ($A$, $B$, $C$).\n\n\\begin{align}T = [| G; A, B , C |],\\end{align}\n\nWe need to solve the following problem for each factor (e.g. factor $A$ here):\n\n\\begin{align}\\min_{A \\geq 0} ||T_{[1]} - A\\times G_{[1]}(B\\times C)^T||_F^2,\\end{align}\n\nHere, $G_{[i]}$ represents ith mode unfolding of the core. To update\nthe core, we need the solve following problem:\n\n\\begin{align}\\min_{g \\geq 0} ||t -   (A\\times B \\times C)\\times g ||_F^2,\\end{align}\n\nwhere $t$ and $g$ are the vectorized data tensor $T$ and core $G$.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To update the factors, we will use HALS and to update the core, we have two\ndifferent algorithms Active Set (AS) and Fast Iterative Shrinkage-Thresholding\nAlgorithm (FISTA) in Tensorly. While FISTA is an accelerated gradient method for\nnon-negative or unconstrained problems, AS is the widely used non-negative\nleast square solution proposed by Lawson and Hanson in 1974. Both algorithms\nreturn non-negative core and FISTA is the default algorithm for HALS Tucker\ndecomposition in Tensorly.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport tensorly as tl\nfrom tensorly.decomposition import non_negative_tucker, non_negative_tucker_hals\nimport time\nfrom tensorly.metrics.regression import RMSE\nimport matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create synthetic tensor\nThere are several ways to create a tensor with non-negative entries in Tensorly.\nHere we chose to generate a random tensor from the sequence of integers from\n1 to 1000.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# tensor generation\narray = np.random.randint(1000, size=(10, 30, 40))\ntensor = tl.tensor(array, dtype='float')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-negative Tucker\nFirst, multiplicative update can be implemented as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "tic = time.time()\ntensor_mu, error_mu = non_negative_tucker(tensor, rank=[5, 5, 5], tol=1e-12, n_iter_max=100, return_errors=True)\ntucker_reconstruction_mu = tl.tucker_to_tensor(tensor_mu)\ntime_mu = time.time()-tic"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here, we also compute the output tensor from the decomposed factors by using\nthe ``tucker_to_tensor`` function. The tensor ``tucker_reconstruction_mu`` is\ntherefore a low-rank non-negative approximation of the input tensor ``tensor``.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-negative Tucker with HALS and FISTA\nHALS algorithm with FISTA can be calculated as:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ticnew = time.time()\ntensor_hals_fista, error_fista = non_negative_tucker_hals(tensor, rank=[5, 5, 5], algorithm='fista', return_errors=True)\ntucker_reconstruction_fista = tl.tucker_to_tensor(tensor_hals_fista)\ntime_fista = time.time()-ticnew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Non-negative Tucker with HALS and Active Set\nAs a second option, HALS algorithm with Active Set can be called as follows:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "ticnew = time.time()\ntensor_hals_as, error_as = non_negative_tucker_hals(tensor, rank=[5, 5, 5], algorithm='active_set', return_errors=True)\ntucker_reconstruction_as = tl.tucker_to_tensor(tensor_hals_as)\ntime_as = time.time()-ticnew"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison\nTo compare the various methods, first we may look at each algorithm\nprocessing time:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('time for tensorly nntucker:'+' ' + str(\"{:.2f}\".format(time_mu)))\nprint('time for HALS with fista:'+' ' + str(\"{:.2f}\".format(time_fista)))\nprint('time for HALS with as:'+' ' + str(\"{:.2f}\".format(time_as)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All algorithms should run with about the same number of iterations on our\nexample, so at first glance the MU algorithm is faster (i.e. has lower\nper-iteration complexity). A second way to compare methods is to compute\nthe error between the output and input tensor. In Tensorly, there is a function\nto compute Root Mean Square Error (RMSE):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print('RMSE tensorly nntucker:'+' ' + str(RMSE(tensor, tucker_reconstruction_mu)))\nprint('RMSE for hals with fista:'+' ' + str(RMSE(tensor, tucker_reconstruction_fista)))\nprint('RMSE for hals with as:'+' ' + str(RMSE(tensor, tucker_reconstruction_as)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "According to the RMSE results, HALS is better than the multiplicative update\nwith both FISTA and active set core update options. We can better appreciate\nthe difference in convergence speed on the following error per iteration plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def each_iteration(a,b,c,title):\n    fig=plt.figure()\n    fig.set_size_inches(10, fig.get_figheight(), forward=True)\n    plt.plot(a)\n    plt.plot(b)\n    plt.plot(c)\n    plt.title(str(title))\n    plt.legend(['MU', 'HALS + Fista', 'HALS + AS'], loc='upper right')\n\n\neach_iteration(error_mu, error_fista, error_as, 'Error for each iteration')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "In conclusion, on this quick test, it appears that the HALS algorithm gives\nmuch better results than the MU original Tensorly methods. Our recommendation\nis to use HALS as a default, and only resort to MU in specific cases\n(only encountered by expert users most likely). Besides, in this experiment\nFISTA and active set give very similar results, however active set may last\nlonger when it is used with higher ranks according to our experience.\nTherefore, we recommend to use FISTA with high rank decomposition.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\nGillis, N., & Glineur, F. (2012). Accelerated multiplicative updates and\nhierarchical ALS algorithms for nonnegative matrix factorization.\nNeural computation, 24(4), 1085-1105. (Link)\n<https://direct.mit.edu/neco/article/24/4/1085/7755/Accelerated-Multiplicative-Updates-and>\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
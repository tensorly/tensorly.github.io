{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Demonstration of PARAFAC2\n\nExample of how to use the PARAFAC2 algorithm.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\nimport numpy.linalg as la\nimport matplotlib.pyplot as plt\nimport tensorly as tl\nfrom tensorly.decomposition import parafac2\nfrom scipy.optimize import linear_sum_assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create synthetic tensor\nHere, we create a random tensor that follows the PARAFAC2 constraints found\ninx `(Kiers et al 1999)`_.\n\nThis particular tensor,\n$\\mathcal{X}\u00a0\\in \\mathbb{R}^{I\\times J \\times K}$, is a shifted\nCP tensor, that is, a tensor on the form:\n\n\\begin{align}\\mathcal{X}_{ijk} = \\sum_{r=1}^R A_{ir} B_{\\sigma_i(j) r} C_{kr},\\end{align}\n\nwhere $\\sigma_i$\u00a0is a cyclic permutation of $J$ elements.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Set parameters\ntrue_rank = 3\nI, J, K = 30, 40, 20\nnoise_rate = 0.1\nnp.random.seed(0)\n\n# Generate random matrices\nA_factor_matrix = np.random.uniform(1, 2, size=(I, true_rank))\nB_factor_matrix = np.random.uniform(size=(J, true_rank))\nC_factor_matrix = np.random.uniform(size=(K, true_rank))\n\n# Normalised factor matrices\nA_normalised = A_factor_matrix/la.norm(A_factor_matrix, axis=0)\nB_normalised = B_factor_matrix/la.norm(B_factor_matrix, axis=0)\nC_normalised = C_factor_matrix/la.norm(C_factor_matrix, axis=0)\n\n# Generate the shifted factor matrix\nB_factor_matrices = [np.roll(B_factor_matrix, shift=i, axis=0) for i in range(I)]\nBs_normalised = [np.roll(B_normalised, shift=i, axis=0) for i in range(I)]\n\n# Construct the tensor\ntensor = np.einsum('ir,ijr,kr->ijk', A_factor_matrix, B_factor_matrices, C_factor_matrix)\n\n# Add noise\nnoise = np.random.standard_normal(tensor.shape)\nnoise /= np.linalg.norm(noise)\nnoise *= noise_rate*np.linalg.norm(tensor)\ntensor += noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fit a PARAFAC2 tensor\nTo avoid local minima, we initialise and fit 10 models and choose the one\nwith the lowest error\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "best_err = np.inf\ndecomposition = None\n\nfor run in range(10):\n    print(f'Training model {run}...')\n    trial_decomposition, trial_errs = parafac2(tensor, true_rank, return_errors=True, tol=1e-8, n_iter_max=500, random_state=run)\n    print(f'Number of iterations: {len(trial_errs)}')\n    print(f'Final error: {trial_errs[-1]}')\n    if best_err > trial_errs[-1]:\n        best_err = trial_errs[-1]\n        err = trial_errs\n        decomposition = trial_decomposition\n    print('-------------------------------')\nprint(f'Best model error: {best_err}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A decomposition is a wrapper object for three variables: the *weights*, \nthe *factor matrices* and the *projection matrices*. The weights are similar\nto the output of a CP decomposition. The factor matrices and projection \nmatrices are somewhat different. For a CP decomposition, we only have the\nweights and the factor matrices. However, since the PARAFAC2 factor matrices\nfor the second mode is given by\n\n\\begin{align}B_i = P_i B,\\end{align}\n\nwhere $B$ is an $R \\times R$ matrix and $P_i$ is an \n$I \\times R$ projection matrix, we cannot store the factor matrices\nthe same as for a CP decomposition.\n\nInstead, we store the factor matrix along the first mode ($A$), the \n\"blueprint\" matrix for the second mode ($B$) and the factor matrix \nalong the third mode ($C$) in one tuple and the projection matrices,\n$P_i$, in a separate tuple.\n\nIf we wish to extract the informative $B_i$ factor matrices, then we\nuse the ``tensorly.parafac2_tensor.apply_projection_matrices`` function on \nthe PARAFAC2 tensor instance to get another wrapper object for two\nvariables: *weights* and *factor matrices*. However, now, the second element\nof the factor matrices tuple is now a list of factor matrices, one for each\nfrontal slice of the tensor.\n\nLikewise, if we wish to construct the tensor or the frontal slices, then we\ncan use the ``tensorly.parafac2_tensor.parafac2_to_tensor`` function. If the\ndecomposed dataset consisted of uneven-length frontal slices, then we can\nuse the ``tensorly.parafac2_tensor.parafac2_to_slices`` function to get a \nlist of frontal slices.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "est_tensor = tl.parafac2_tensor.parafac2_to_tensor(decomposition)\nest_weights, (est_A, est_B, est_C) = tl.parafac2_tensor.apply_parafac2_projections(decomposition)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compute performance metrics\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "reconstruction_error = la.norm(est_tensor - tensor)\nrecovery_rate = 1 - reconstruction_error/la.norm(tensor)\n\nprint(f'{recovery_rate:2.0%} of the data is explained by the model, which is expected with noise rate: {noise_rate}')\n\n\n# To evaluate how well the original structure is recovered, we calculate the tucker congruence coefficient.\n\nest_A, est_projected_Bs, est_C = tl.parafac2_tensor.apply_parafac2_projections(decomposition)[1]\n\nsign = np.sign(est_A)\nest_A = np.abs(est_A)\nest_projected_Bs = sign[:, np.newaxis]*est_projected_Bs\n\nest_A_normalised = est_A/la.norm(est_A, axis=0)\nest_Bs_normalised = [est_B/la.norm(est_B, axis=0) for est_B in est_projected_Bs]\nest_C_normalised = est_C/la.norm(est_C, axis=0)\n\nB_corr = np.array(est_Bs_normalised).reshape(-1, true_rank).T@np.array(Bs_normalised).reshape(-1, true_rank)/len(est_Bs_normalised)\nA_corr = est_A_normalised.T@A_normalised\nC_corr = est_C_normalised.T@C_normalised\n\ncorr = A_corr*B_corr*C_corr\npermutation = linear_sum_assignment(-corr)  # Old versions of scipy does not support maximising, from scipy v1.4, you can pass `corr` and `maximize=True` instead of `-corr` to maximise the sum.\n\ncongruence_coefficient = np.mean(corr[permutation])\nprint(f'Average tucker congruence coefficient: {congruence_coefficient}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualize the components\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Find the best permutation so that we can plot the estimated components on top of the true components\npermutation = np.argmax(A_corr*B_corr*C_corr, axis=0)\n\n\n# Create plots of each component vector for each mode\n# (We just look at one of the B_i matrices)\n\nfig, axes = plt.subplots(true_rank, 3, figsize=(15, 3*true_rank+1))\ni = 0 # What slice, B_i, we look at for the B mode\n\nfor r in range(true_rank):\n    \n    # Plot true and estimated components for mode A\n    axes[r][0].plot((A_normalised[:, r]), label='True')\n    axes[r][0].plot((est_A_normalised[:, permutation[r]]),'--', label='Estimated')\n    \n    # Labels for the different components\n    axes[r][0].set_ylabel(f'Component {r}')\n\n    # Plot true and estimated components for mode C\n    axes[r][2].plot(C_normalised[:, r])\n    axes[r][2].plot(est_C_normalised[:, permutation[r]], '--')\n\n    # Plot true components for mode B\n    axes[r][1].plot(Bs_normalised[i][:, r])\n    \n    # Get the signs so that we can flip the B mode factor matrices\n    A_sign = np.sign(est_A_normalised)\n    \n    # Plot estimated components for mode B (after sign correction)\n    axes[r][1].plot(A_sign[i, r]*est_Bs_normalised[i][:, permutation[r]], '--')\n\n# Titles for the different modes\naxes[0][0].set_title('A mode')\naxes[0][2].set_title('C mode')\naxes[0][1].set_title(f'B mode (slice {i})')\n\n# Create a legend for the entire figure  \nhandles, labels =  axes[r][0].get_legend_handles_labels()\nfig.legend(handles, labels, loc='upper center', ncol=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspect the convergence rate\nIt can be interesting to look at the loss plot to make sure that we have\nconverged to a stationary point. We skip the first iteration since the\ninitial loss often dominate the rest of the plot, making it difficult\nto check for convergence.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "loss_fig, loss_ax = plt.subplots(figsize=(9, 9/1.6))\nloss_ax.plot(range(1, len(err)), err[1:])\nloss_ax.set_xlabel('Iteration number')\nloss_ax.set_ylabel('Relative reconstruction error')\nmathematical_expression_of_loss = r\"$\\frac{\\left|\\left|\\hat{\\mathcal{X}}\\right|\\right|_F}{\\left|\\left|\\mathcal{X}\\right|\\right|_F}$\"\nloss_ax.set_title(f'Loss plot: {mathematical_expression_of_loss} \\n (starting after first iteration)', fontsize=16)\nxticks = loss_ax.get_xticks()\nloss_ax.set_xticks([1] + list(xticks[1:]))\nloss_ax.set_xlim(1, len(err))\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## References\n\n\nKiers HA, Ten Berge JM, Bro R. *PARAFAC2\u2014Part I. \nA direct fitting algorithm for the PARAFAC2 model.*\n**Journal of Chemometrics: A Journal of the Chemometrics Society.**\n1999 May;13(3\u20104):275-94. [(Online version)](https://onlinelibrary.wiley.com/doi/abs/10.1002/(SICI)1099-128X(199905/08)13:3/4%3C275::AID-CEM543%3E3.0.CO;2-B)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
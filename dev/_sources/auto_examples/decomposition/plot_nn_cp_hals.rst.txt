
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/decomposition/plot_nn_cp_hals.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        :ref:`Go to the end <sphx_glr_download_auto_examples_decomposition_plot_nn_cp_hals.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_decomposition_plot_nn_cp_hals.py:


Non-negative CP decomposition in Tensorly >=0.6
===============================================
Example and comparison of Non-negative Parafac decompositions.

.. GENERATED FROM PYTHON SOURCE LINES 8-20

Introduction
-----------------------
Since version 0.6 in Tensorly, several options are available to compute
non-negative CP (NCP), in particular several
algorithms:

1. Multiplicative updates (MU) (already in Tensorly < 0.6)
2. Non-negative Alternating Least Squares (ALS) using Hierarchical ALS (HALS)

Non-negativity is an important constraint to handle for tensor decompositions.
One could expect that factors must have only non-negative values after it is
obtained from a non-negative tensor.

.. GENERATED FROM PYTHON SOURCE LINES 20-29

.. code-block:: default


    import numpy as np
    import tensorly as tl
    from tensorly.decomposition import non_negative_parafac, non_negative_parafac_hals
    from tensorly.decomposition._cp import initialize_cp
    from tensorly.cp_tensor import CPTensor
    import time
    from copy import deepcopy








.. GENERATED FROM PYTHON SOURCE LINES 30-34

Create synthetic tensor
-----------------------
There are several ways to create a tensor with non-negative entries in Tensorly.
Here we chose to generate a random from the sequence of integers from 1 to 24000.

.. GENERATED FROM PYTHON SOURCE LINES 34-38

.. code-block:: default


    # Tensor generation
    tensor = tl.tensor(np.arange(24000).reshape((30, 40, 20)), dtype=tl.float32)








.. GENERATED FROM PYTHON SOURCE LINES 39-48

Our goal here is to produce an approximation of the tensor generated above
which follows a low-rank CP model, with non-negative coefficients. Before
using these algorithms, we can use Tensorly to produce a good initial guess
for our NCP. In fact, in order to compare both algorithmic options in a
fair way, it is a good idea to use same initialized factors in decomposition
algorithms. We make use of the ``initialize_cp`` function to initialize the
factors of the NCP (setting the ``non_negative`` option to `True`) 
and transform these factors (and factors weights) into
an instance of the CPTensor class:

.. GENERATED FROM PYTHON SOURCE LINES 48-53

.. code-block:: default


    weights_init, factors_init = initialize_cp(tensor, non_negative=True, init='random', rank=10)

    cp_init = CPTensor((weights_init, factors_init))








.. GENERATED FROM PYTHON SOURCE LINES 54-59

Non-negative Parafac
-----------------------
From now on, we can use the same ``cp_init`` tensor as the initial tensor when
we use decomposition functions. Now let us first use the algorithm based on
Multiplicative Update, which can be called as follows:

.. GENERATED FROM PYTHON SOURCE LINES 59-65

.. code-block:: default


    tic = time.time()
    tensor_mu, errors_mu = non_negative_parafac(tensor, rank=10, init=deepcopy(cp_init), return_errors=True)
    cp_reconstruction_mu = tl.cp_to_tensor(tensor_mu)
    time_mu = time.time()-tic








.. GENERATED FROM PYTHON SOURCE LINES 66-71

Here, we also compute the output tensor from the decomposed factors by using
the cp_to_tensor function. The tensor cp_reconstruction_mu is therefore a
low-rank non-negative approximation of the input tensor; looking at the
first few values of both tensors shows that this is indeed
the case but the approximation is quite coarse.

.. GENERATED FROM PYTHON SOURCE LINES 71-75

.. code-block:: default


    print('reconstructed tensor\n', cp_reconstruction_mu[10:12, 10:12, 10:12], '\n')
    print('input data tensor\n', tensor[10:12, 10:12, 10:12])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    reconstructed tensor
     [[[8279.57 8142.17]
      [8495.03 8238.47]]

     [[8436.18 8834.58]
      [8693.99 8918.5 ]]] 

    input data tensor
     [[[8210. 8211.]
      [8230. 8231.]]

     [[9010. 9011.]
      [9030. 9031.]]]




.. GENERATED FROM PYTHON SOURCE LINES 76-80

Non-negative Parafac with HALS
------------------------------
Our second (new) option to compute NCP is the HALS algorithm, which can be
used as follows:

.. GENERATED FROM PYTHON SOURCE LINES 80-86

.. code-block:: default


    tic = time.time()
    tensor_hals, errors_hals = non_negative_parafac_hals(tensor, rank=10, init=deepcopy(cp_init), return_errors=True)
    cp_reconstruction_hals = tl.cp_to_tensor(tensor_hals)
    time_hals = time.time()-tic








.. GENERATED FROM PYTHON SOURCE LINES 87-88

Again, we can look at the reconstructed tensor entries.

.. GENERATED FROM PYTHON SOURCE LINES 88-92

.. code-block:: default


    print('reconstructed tensor\n',cp_reconstruction_hals[10:12, 10:12, 10:12], '\n')
    print('input data tensor\n', tensor[10:12, 10:12, 10:12])





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    reconstructed tensor
     [[[8212.77 8195.27]
      [8221.18 8229.13]]

     [[9012.66 8997.44]
      [9022.11 9029.5 ]]] 

    input data tensor
     [[[8210. 8211.]
      [8230. 8231.]]

     [[9010. 9011.]
      [9030. 9031.]]]




.. GENERATED FROM PYTHON SOURCE LINES 93-103

Non-negative Parafac with Exact HALS
------------------------------------
From only looking at a few entries of the reconstructed tensors, we can
already see a huge gap between HALS and MU outputs.
Additionally, HALS algorithm has an option for exact solution to the non-negative
least squares subproblem rather than the faster, approximate solution.
Note that the overall HALS algorithm will still provide an approximation of
the input data, but will need longer to reach convergence.
Exact subroutine solution option can be used simply choosing exact as True
in the function:

.. GENERATED FROM PYTHON SOURCE LINES 103-109

.. code-block:: default


    tic = time.time()
    tensorhals_exact, errors_exact = non_negative_parafac_hals(tensor, rank=10, init=deepcopy(cp_init), return_errors=True, exact=True)
    cp_reconstruction_exact_hals = tl.cp_to_tensor(tensorhals_exact)
    time_exact_hals = time.time()-tic








.. GENERATED FROM PYTHON SOURCE LINES 110-113

Comparison
-----------------------
First comparison option is processing time for each algorithm:

.. GENERATED FROM PYTHON SOURCE LINES 113-118

.. code-block:: default


    print(str("{:.2f}".format(time_mu)) + ' ' + 'seconds')
    print(str("{:.2f}".format(time_hals)) + ' ' + 'seconds')
    print(str("{:.2f}".format(time_exact_hals)) + ' ' + 'seconds')





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    0.20 seconds
    0.12 seconds
    371.85 seconds




.. GENERATED FROM PYTHON SOURCE LINES 119-126

As it is expected, the exact solution takes much longer than the approximate
solution, while the gain in performance is often void. Therefore we recommend
to avoid this option unless it is specifically required by the application.
Also note that on appearance, both MU and HALS have similar runtimes.
However, a closer look suggest they are indeed behaving quite differently.
Computing the error between the output and the input tensor tells that story better.
In Tensorly, we provide a function to calculate Root Mean Square Error (RMSE):

.. GENERATED FROM PYTHON SOURCE LINES 126-132

.. code-block:: default


    from tensorly.metrics.regression import RMSE
    print(RMSE(tensor, cp_reconstruction_mu))
    print(RMSE(tensor, cp_reconstruction_hals))
    print(RMSE(tensor, cp_reconstruction_exact_hals))





.. rst-class:: sphx-glr-script-out

 .. code-block:: none

    221.41252
    25.331636
    0.4651501




.. GENERATED FROM PYTHON SOURCE LINES 133-137

According to the RMSE results, HALS is better than the multiplicative update
with both exact and approximate solution. In particular, HALS converged to a
much lower reconstruction error than MU. We can better appreciate the difference
in convergence speed on the following error per iteration plot:

.. GENERATED FROM PYTHON SOURCE LINES 137-151

.. code-block:: default


    import matplotlib.pyplot as plt
    def each_iteration(a,b,c,title):
        fig=plt.figure()
        fig.set_size_inches(10, fig.get_figheight(), forward=True)
        plt.plot(a)
        plt.plot(b)
        plt.plot(c)
        plt.title(str(title))
        plt.legend(['MU', 'HALS', 'Exact HALS'], loc='upper left')


    each_iteration(errors_mu, errors_hals, errors_exact, 'Error for each iteration')




.. image-sg:: /auto_examples/decomposition/images/sphx_glr_plot_nn_cp_hals_001.png
   :alt: Error for each iteration
   :srcset: /auto_examples/decomposition/images/sphx_glr_plot_nn_cp_hals_001.png
   :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 152-156

In conclusion, on this quick test, it appears that the HALS algorithm gives
much better results than the MU original Tensorly methods. Our recommendation
is to use HALS as a default, and only resort to MU in specific cases (only
encountered by expert users most likely).

.. GENERATED FROM PYTHON SOURCE LINES 158-165

References
----------

Gillis, N., & Glineur, F. (2012). Accelerated multiplicative updates and
hierarchical ALS algorithms for nonnegative matrix factorization.
Neural computation, 24(4), 1085-1105. (Link)
<https://direct.mit.edu/neco/article/24/4/1085/7755/Accelerated-Multiplicative-Updates-and>


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 6 minutes  12.283 seconds)


.. _sphx_glr_download_auto_examples_decomposition_plot_nn_cp_hals.py:

.. only:: html

  .. container:: sphx-glr-footer sphx-glr-footer-example




    .. container:: sphx-glr-download sphx-glr-download-python

      :download:`Download Python source code: plot_nn_cp_hals.py <plot_nn_cp_hals.py>`

    .. container:: sphx-glr-download sphx-glr-download-jupyter

      :download:`Download Jupyter notebook: plot_nn_cp_hals.ipynb <plot_nn_cp_hals.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
